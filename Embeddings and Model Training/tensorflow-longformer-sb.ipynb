{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TensorFlow LongFormer NER Baseline \n\nThis notebook is a TensorFlow starter notebook for Kaggle's \"Feedback Prize - Evaluating Student Writing\" Competition. Currently this notebook uses\n* backbone LongFormer\n* NER formulation\n* one fold\n\nWith simple changes, we can convert this notebook into Question Answer formulation and we can try different backbones. Furthermore this notebook is one fold. It trains with 90% data and validates on 10% data. We can convert this notebook to K-fold or train with 100% data for boost in LB.","metadata":{}},{"cell_type":"code","source":"import os\n# DECLARE HOW MANY GPUS YOU WISH TO USE. \n# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n\n# VERSION FOR SAVING/LOADING MODEL WEIGHTS\n# THIS SHOULD MATCH THE MODEL IN LOAD_MODEL_FROM\nVER=14 \n\n# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\nLOAD_TOKENS_FROM = '../input/tf-longformer-v12'\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\nLOAD_MODEL_FROM = '../input/tflongformerv14'\n\n# IF FOLLOWING IS NONE, THEN NOTEBOOK \n# USES INTERNET AND DOWNLOADS HUGGINGFACE \n# CONFIG, TOKENIZER, AND MODEL\nDOWNLOADED_MODEL_PATH = '../input/tf-longformer-v12'\n\nif DOWNLOADED_MODEL_PATH is None:\n    DOWNLOADED_MODEL_PATH = 'model'    \nMODEL_NAME = 'allenai/longformer-base-4096'","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:25.272376Z","iopub.execute_input":"2023-05-06T06:40:25.273397Z","iopub.status.idle":"2023-05-06T06:40:25.297481Z","shell.execute_reply.started":"2023-05-06T06:40:25.273274Z","shell.execute_reply":"2023-05-06T06:40:25.296718Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"if DOWNLOADED_MODEL_PATH == 'model':\n    os.mkdir('model')\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained('model')\n\n    config = AutoConfig.from_pretrained(MODEL_NAME) \n    config.save_pretrained('model')\n\n    backbone = TFAutoModel.from_pretrained(MODEL_NAME, config=config)\n    backbone.save_pretrained('model')","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:25.299269Z","iopub.execute_input":"2023-05-06T06:40:25.299745Z","iopub.status.idle":"2023-05-06T06:40:25.304837Z","shell.execute_reply.started":"2023-05-06T06:40:25.299705Z","shell.execute_reply":"2023-05-06T06:40:25.304030Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"The above saves the files\n* TOKENIZER FILES - merges.txt, tokenizer_config.json, special_tokens_map.json, tokenizer.json, vocab.json\n* CONFIG FILE - config.json\n* MODEL WEIGHT FILE - tf_model.h5\n\nThen just upload all these files to a Kaggle dataset, like what I did [here][1]. Then you load them into your notebook like the notebook you are reading. And we can turn internet off!\n\n[1]: https://www.kaggle.com/cdeotte/tf-longformer-v12","metadata":{}},{"cell_type":"markdown","source":"# Load Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom transformers import *\nprint('TF version',tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:25.306390Z","iopub.execute_input":"2023-05-06T06:40:25.306877Z","iopub.status.idle":"2023-05-06T06:40:35.866868Z","shell.execute_reply.started":"2023-05-06T06:40:25.306840Z","shell.execute_reply":"2023-05-06T06:40:35.865316Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"TF version 2.6.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# USE MULTIPLE GPUS\nif os.environ[\"CUDA_VISIBLE_DEVICES\"].count(',') == 0:\n    strategy = tf.distribute.get_strategy()\n    print('single strategy')\nelse:\n    strategy = tf.distribute.MirroredStrategy()\n    print('multiple strategy')","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:35.869171Z","iopub.execute_input":"2023-05-06T06:40:35.869449Z","iopub.status.idle":"2023-05-06T06:40:35.883249Z","shell.execute_reply.started":"2023-05-06T06:40:35.869414Z","shell.execute_reply":"2023-05-06T06:40:35.882399Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"single strategy\n","output_type":"stream"}]},{"cell_type":"code","source":"tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\nprint('Mixed precision enabled')","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:35.884615Z","iopub.execute_input":"2023-05-06T06:40:35.885335Z","iopub.status.idle":"2023-05-06T06:40:35.890416Z","shell.execute_reply.started":"2023-05-06T06:40:35.885298Z","shell.execute_reply":"2023-05-06T06:40:35.889689Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Mixed precision enabled\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Train","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/feedback-prize-2021/train.csv')\nprint( train.shape )\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:35.891740Z","iopub.execute_input":"2023-05-06T06:40:35.892498Z","iopub.status.idle":"2023-05-06T06:40:37.397755Z","shell.execute_reply.started":"2023-05-06T06:40:35.892454Z","shell.execute_reply":"2023-05-06T06:40:37.397005Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(144293, 8)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"             id  discourse_id  discourse_start  discourse_end  \\\n0  423A1CA112E2  1.622628e+12              8.0          229.0   \n1  423A1CA112E2  1.622628e+12            230.0          312.0   \n2  423A1CA112E2  1.622628e+12            313.0          401.0   \n3  423A1CA112E2  1.622628e+12            402.0          758.0   \n4  423A1CA112E2  1.622628e+12            759.0          886.0   \n\n                                      discourse_text discourse_type  \\\n0  Modern humans today are always on their phone....           Lead   \n1  They are some really bad consequences when stu...       Position   \n2  Some certain areas in the United States ban ph...       Evidence   \n3  When people have phones, they know about certa...       Evidence   \n4  Driving is one of the way how to get around. P...          Claim   \n\n  discourse_type_num                                   predictionstring  \n0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>discourse_id</th>\n      <th>discourse_start</th>\n      <th>discourse_end</th>\n      <th>discourse_text</th>\n      <th>discourse_type</th>\n      <th>discourse_type_num</th>\n      <th>predictionstring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>8.0</td>\n      <td>229.0</td>\n      <td>Modern humans today are always on their phone....</td>\n      <td>Lead</td>\n      <td>Lead 1</td>\n      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>230.0</td>\n      <td>312.0</td>\n      <td>They are some really bad consequences when stu...</td>\n      <td>Position</td>\n      <td>Position 1</td>\n      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>313.0</td>\n      <td>401.0</td>\n      <td>Some certain areas in the United States ban ph...</td>\n      <td>Evidence</td>\n      <td>Evidence 1</td>\n      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>402.0</td>\n      <td>758.0</td>\n      <td>When people have phones, they know about certa...</td>\n      <td>Evidence</td>\n      <td>Evidence 2</td>\n      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>759.0</td>\n      <td>886.0</td>\n      <td>Driving is one of the way how to get around. P...</td>\n      <td>Claim</td>\n      <td>Claim 1</td>\n      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('The train labels are:')\ntrain.discourse_type.unique()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:37.399205Z","iopub.execute_input":"2023-05-06T06:40:37.399683Z","iopub.status.idle":"2023-05-06T06:40:37.418823Z","shell.execute_reply.started":"2023-05-06T06:40:37.399646Z","shell.execute_reply":"2023-05-06T06:40:37.418155Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"The train labels are:\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array(['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n       'Counterclaim', 'Rebuttal'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"IDS = train.id.unique()\nprint('There are',len(IDS),'train texts.')","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:37.420151Z","iopub.execute_input":"2023-05-06T06:40:37.420586Z","iopub.status.idle":"2023-05-06T06:40:37.437846Z","shell.execute_reply.started":"2023-05-06T06:40:37.420549Z","shell.execute_reply":"2023-05-06T06:40:37.437019Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"There are 15594 train texts.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Tokenize Train\nThe following code converts Kaggle's train dataset into a NER token array that we can use to train a NER transformer. I have made it very clear which targets belong to which class. This allows us to very easily convert this code to `Question Answer formulation` if we want. Just change the 14 NER arrays to be 14 arrays of `start position` and `end position` for each of the 7 classes. (You will need to think creatively what to do if a single text has multiple of one class).","metadata":{}},{"cell_type":"code","source":"MAX_LEN = 1024\n\n# THE TOKENS AND ATTENTION ARRAYS\ntokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH)\ntrain_tokens = np.zeros((len(IDS),MAX_LEN), dtype='int32')\ntrain_attention = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n\n# THE 14 CLASSES FOR NER\nlead_b = np.zeros((len(IDS),MAX_LEN))\nlead_i = np.zeros((len(IDS),MAX_LEN))\n\nposition_b = np.zeros((len(IDS),MAX_LEN))\nposition_i = np.zeros((len(IDS),MAX_LEN))\n\nevidence_b = np.zeros((len(IDS),MAX_LEN))\nevidence_i = np.zeros((len(IDS),MAX_LEN))\n\nclaim_b = np.zeros((len(IDS),MAX_LEN))\nclaim_i = np.zeros((len(IDS),MAX_LEN))\n\nconclusion_b = np.zeros((len(IDS),MAX_LEN))\nconclusion_i = np.zeros((len(IDS),MAX_LEN))\n\ncounterclaim_b = np.zeros((len(IDS),MAX_LEN))\ncounterclaim_i = np.zeros((len(IDS),MAX_LEN))\n\nrebuttal_b = np.zeros((len(IDS),MAX_LEN))\nrebuttal_i = np.zeros((len(IDS),MAX_LEN))\n\n# HELPER VARIABLES\ntrain_lens = []\ntargets_b = [lead_b, position_b, evidence_b, claim_b, conclusion_b, counterclaim_b, rebuttal_b]\ntargets_i = [lead_i, position_i, evidence_i, claim_i, conclusion_i, counterclaim_i, rebuttal_i]\ntarget_map = {'Lead':0, 'Position':1, 'Evidence':2, 'Claim':3, 'Concluding Statement':4,\n             'Counterclaim':5, 'Rebuttal':6}","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:37.439034Z","iopub.execute_input":"2023-05-06T06:40:37.439786Z","iopub.status.idle":"2023-05-06T06:40:37.588283Z","shell.execute_reply.started":"2023-05-06T06:40:37.439749Z","shell.execute_reply":"2023-05-06T06:40:37.587453Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# WE ASSUME DATAFRAME IS ASCENDING WHICH IT IS\nassert( np.sum(train.groupby('id')['discourse_start'].diff()<=0)==0 )\n\n# FOR LOOP THROUGH EACH TRAIN TEXT\nfor id_num in range(len(IDS)):\n    if LOAD_TOKENS_FROM: break\n    if id_num%100==0: print(id_num,', ',end='')\n        \n    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n    n = IDS[id_num]\n    name = f'../input/feedback-prize-2021/train/{n}.txt'\n    txt = open(name, 'r').read()\n    train_lens.append( len(txt.split()))\n    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n                                   truncation=True, return_offsets_mapping=True)\n    train_tokens[id_num,] = tokens['input_ids']\n    train_attention[id_num,] = tokens['attention_mask']\n    \n    # FIND TARGETS IN TEXT AND SAVE IN TARGET ARRAYS\n    offsets = tokens['offset_mapping']\n    offset_index = 0\n    df = train.loc[train.id==n]\n    for index,row in df.iterrows():\n        a = row.discourse_start\n        b = row.discourse_end\n        if offset_index>len(offsets)-1:\n            break\n        c = offsets[offset_index][0]\n        d = offsets[offset_index][1]\n        beginning = True\n        while b>c:\n            if (c>=a)&(b>=d):\n                k = target_map[row.discourse_type]\n                if beginning:\n                    targets_b[k][id_num][offset_index] = 1\n                    beginning = False\n                else:\n                    targets_i[k][id_num][offset_index] = 1\n            offset_index += 1\n            if offset_index>len(offsets)-1:\n                break\n            c = offsets[offset_index][0]\n            d = offsets[offset_index][1]","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:37.591385Z","iopub.execute_input":"2023-05-06T06:40:37.591643Z","iopub.status.idle":"2023-05-06T06:40:40.227492Z","shell.execute_reply.started":"2023-05-06T06:40:37.591603Z","shell.execute_reply":"2023-05-06T06:40:40.226679Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"if LOAD_TOKENS_FROM is None:\n    plt.hist(train_lens,bins=100)\n    plt.title('Histogram of Train Word Counts',size=16)\n    plt.xlabel('Train Word Count',size=14)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:40.228885Z","iopub.execute_input":"2023-05-06T06:40:40.229153Z","iopub.status.idle":"2023-05-06T06:40:40.237081Z","shell.execute_reply.started":"2023-05-06T06:40:40.229118Z","shell.execute_reply":"2023-05-06T06:40:40.236202Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Dec-2021/lengths4.png)\n\nFrom the histogram of train word counts above, we see that using a transformer width of 1024 is a good comprise of capturing most of the data's signal but not having too large a model. (Note that analyzing a histogram of train **token** counts would be better but we don't do that here). We could probably explore other widths between 512 and 1024 also. Or we could use widths of size 512 or smaller and use a stride which breaks a single text into multiple chunks (with possible overlap).","metadata":{}},{"cell_type":"code","source":"if LOAD_TOKENS_FROM is None:\n    targets = np.zeros((len(IDS),MAX_LEN,15), dtype='int32')\n    for k in range(7):\n        targets[:,:,2*k] = targets_b[k]\n        targets[:,:,2*k+1] = targets_i[k]\n    targets[:,:,14] = 1-np.max(targets,axis=-1)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:40.238271Z","iopub.execute_input":"2023-05-06T06:40:40.238659Z","iopub.status.idle":"2023-05-06T06:40:40.246750Z","shell.execute_reply.started":"2023-05-06T06:40:40.238624Z","shell.execute_reply":"2023-05-06T06:40:40.246036Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"if LOAD_TOKENS_FROM is None:\n    np.save(f'targets_{MAX_LEN}', targets)\n    np.save(f'tokens_{MAX_LEN}', train_tokens)\n    np.save(f'attention_{MAX_LEN}', train_attention)\n    print('Saved NER tokens')\nelse:\n    targets = np.load(f'{LOAD_TOKENS_FROM}/targets_{MAX_LEN}.npy')\n    train_tokens = np.load(f'{LOAD_TOKENS_FROM}/tokens_{MAX_LEN}.npy')\n    train_attention = np.load(f'{LOAD_TOKENS_FROM}/attention_{MAX_LEN}.npy')\n    print('Loaded NER tokens')","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:40.248205Z","iopub.execute_input":"2023-05-06T06:40:40.248455Z","iopub.status.idle":"2023-05-06T06:40:51.073676Z","shell.execute_reply.started":"2023-05-06T06:40:40.248422Z","shell.execute_reply":"2023-05-06T06:40:51.072071Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Loaded NER tokens\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Build Model\nWe will use LongFormer backbone and add our own NER head using one hidden layer of size 256 and one final layer with softmax. We use 15 classes because we have a `B` class and `I` class for each of 7 labels. And we have an additional class (called `O` class) for tokens that do not belong to one of the 14 classes.","metadata":{}},{"cell_type":"code","source":"def build_model():\n    \n    tokens = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'tokens', dtype=tf.int32)\n    attention = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'attention', dtype=tf.int32)\n    \n    config = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json') \n    backbone = TFAutoModel.from_pretrained(DOWNLOADED_MODEL_PATH+'/tf_model.h5', config=config)\n    \n    x = backbone(tokens, attention_mask=attention)\n    x = tf.keras.layers.Dense(256, activation='relu')(x[0])\n    x = tf.keras.layers.Dense(15, activation='softmax', dtype='float32')(x)\n    \n    model = tf.keras.Model(inputs=[tokens,attention], outputs=x)\n    model.compile(optimizer = tf.keras.optimizers.Adam(lr = 1e-4),\n                  loss = [tf.keras.losses.CategoricalCrossentropy()],\n                  metrics = [tf.keras.metrics.CategoricalAccuracy()])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:51.074983Z","iopub.execute_input":"2023-05-06T06:40:51.075244Z","iopub.status.idle":"2023-05-06T06:40:51.082757Z","shell.execute_reply.started":"2023-05-06T06:40:51.075209Z","shell.execute_reply":"2023-05-06T06:40:51.082047Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = build_model()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:40:51.085096Z","iopub.execute_input":"2023-05-06T06:40:51.085552Z","iopub.status.idle":"2023-05-06T06:41:22.005098Z","shell.execute_reply.started":"2023-05-06T06:40:51.085515Z","shell.execute_reply":"2023-05-06T06:41:22.004323Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"All model checkpoint layers were used when initializing TFLongformerModel.\n\nAll the layers of TFLongformerModel were initialized from the model checkpoint at ../input/tf-longformer-v12/tf_model.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train or Load Model\nIf you provide a path in variable `LOAD_MODEL_FROM` above, then it will load your previously trained model. Otherwise it will train now. \n\nThe model which is loaded in versions 1 thru 4 of this notebook were trained offline using 5 epochs of batch size 32 and learning rate `1e4` for the first four and `1e5` for the last epoch. That model was trained using 4xV100 GPU. \n\nVersion 5 of this notebook loads a model that was trained in a Kaggle notebook by user @kaggleqrdl in his notebook version 20 [here][1]. Please upvote Qrdl's notebook :-) Qrdl has been running experiments and found a great learning rate to use when training with small batch size.\n\nWhen training on Kaggle's 1xP100 GPU, we need to reduce the batch size to 4. And we reduce the learning rates to `0.25e-4` and `0.25e-5`. I have updated the batch size and learning rate below so that it is ready to train in a Kaggle notebook! (Each training epoch on Kaggle takes 1 hour 8 minutes).\n\n[1]: https://www.kaggle.com/kaggleqrdl/v4expmt-tensorflow-longformer-ner-cv-0-634?scriptVersionId=83341823","metadata":{}},{"cell_type":"code","source":"# LEARNING RATE SCHEDULE AND MODEL CHECKPOINT\nEPOCHS = 5\nBATCH_SIZE = 4 \nLRS = [0.25e-4, 0.25e-4, 0.25e-4, 0.25e-4, 0.25e-5] \ndef lrfn(epoch):\n    return LRS[epoch]\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:41:22.006377Z","iopub.execute_input":"2023-05-06T06:41:22.008147Z","iopub.status.idle":"2023-05-06T06:41:22.013301Z","shell.execute_reply.started":"2023-05-06T06:41:22.008103Z","shell.execute_reply":"2023-05-06T06:41:22.012499Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# TRAIN VALID SPLIT 90% 10%\nnp.random.seed(42)\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\nnp.random.seed(None)\nprint('Train size',len(train_idx),', Valid size',len(valid_idx))","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:41:22.014774Z","iopub.execute_input":"2023-05-06T06:41:22.015060Z","iopub.status.idle":"2023-05-06T06:41:22.027647Z","shell.execute_reply.started":"2023-05-06T06:41:22.015024Z","shell.execute_reply":"2023-05-06T06:41:22.026856Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Train size 14034 , Valid size 1560\n","output_type":"stream"}]},{"cell_type":"code","source":"# LOAD MODEL\nif LOAD_MODEL_FROM:\n    model.load_weights(f'{LOAD_MODEL_FROM}/long_v{VER}.h5')\n    \n# OR TRAIN MODEL\nelse:\n    model.fit(x = [train_tokens[train_idx,], train_attention[train_idx,]],\n          y = targets[train_idx,],\n          validation_data = ([train_tokens[valid_idx,], train_attention[valid_idx,]],\n                             targets[valid_idx,]),\n          callbacks = [lr_callback],\n          epochs = EPOCHS,\n          batch_size = BATCH_SIZE,\n          verbose = 2)\n\n    # SAVE MODEL WEIGHTS\n    model.save_weights(f'long_v{VER}.h5')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-05-06T06:41:22.029289Z","iopub.execute_input":"2023-05-06T06:41:22.029712Z","iopub.status.idle":"2023-05-06T06:41:31.492180Z","shell.execute_reply.started":"2023-05-06T06:41:22.029676Z","shell.execute_reply":"2023-05-06T06:41:31.491374Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Validate Model - Infer OOF\nWe will now make predictions on the validation texts. Our model makes label predictions for each token, we need to convert this into a list of word indices for each label. Note that the tokens and words are not the same. A single word may be broken into multiple tokens. Therefore we need to first create a map to change token indices to word indices.","metadata":{}},{"cell_type":"code","source":"p = model.predict([train_tokens[valid_idx,], train_attention[valid_idx,]], \n                  batch_size=16, verbose=2)\nprint('OOF predictions shape:',p.shape)\noof_preds = np.argmax(p,axis=-1)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:41:31.493522Z","iopub.execute_input":"2023-05-06T06:41:31.493785Z","iopub.status.idle":"2023-05-06T06:44:30.087957Z","shell.execute_reply.started":"2023-05-06T06:41:31.493751Z","shell.execute_reply":"2023-05-06T06:44:30.087208Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"98/98 - 178s\nOOF predictions shape: (1560, 1024, 15)\n","output_type":"stream"}]},{"cell_type":"code","source":"# from tensorflow.keras.models import load_model\n# import transformers\n# ner_model = load_model('/kaggle/working/longformer_ner_w.h5')","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:44:30.089335Z","iopub.execute_input":"2023-05-06T06:44:30.089745Z","iopub.status.idle":"2023-05-06T06:44:30.093479Z","shell.execute_reply.started":"2023-05-06T06:44:30.089708Z","shell.execute_reply":"2023-05-06T06:44:30.092708Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"target_map_rev = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:44:30.094765Z","iopub.execute_input":"2023-05-06T06:44:30.095504Z","iopub.status.idle":"2023-05-06T06:44:30.102613Z","shell.execute_reply.started":"2023-05-06T06:44:30.095469Z","shell.execute_reply":"2023-05-06T06:44:30.101887Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def get_preds(dataset='train', verbose=True, text_ids=IDS[valid_idx], preds=oof_preds):\n    all_predictions = []\n\n    for id_num in range(len(preds)):\n    \n        # GET ID\n        if (id_num%100==0)&(verbose): \n            print(id_num,', ',end='')\n        n = text_ids[id_num]\n    \n        # GET TOKEN POSITIONS IN CHARS\n        name = f'../input/feedback-prize-2021/{dataset}/{n}.txt'\n        txt = open(name, 'r').read()\n        tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n                                   truncation=True, return_offsets_mapping=True)\n        off = tokens['offset_mapping']\n    \n        # GET WORD POSITIONS IN CHARS\n        w = []\n        blank = True\n        for i in range(len(txt)):\n            if (txt[i]!=' ')&(txt[i]!='\\n')&(txt[i]!='\\xa0')&(txt[i]!='\\x85')&(blank==True):\n                w.append(i)\n                blank=False\n            elif (txt[i]==' ')|(txt[i]=='\\n')|(txt[i]=='\\xa0')|(txt[i]=='\\x85'):\n                blank=True\n        w.append(1e6)\n            \n        # MAPPING FROM TOKENS TO WORDS\n        word_map = -1 * np.ones(MAX_LEN,dtype='int32')\n        w_i = 0\n        for i in range(len(off)):\n            if off[i][1]==0: continue\n            while off[i][0]>=w[w_i+1]: w_i += 1\n            word_map[i] = int(w_i)\n        \n        # CONVERT TOKEN PREDICTIONS INTO WORD LABELS\n        ### KEY: ###\n        # 0: LEAD_B, 1: LEAD_I\n        # 2: POSITION_B, 3: POSITION_I\n        # 4: EVIDENCE_B, 5: EVIDENCE_I\n        # 6: CLAIM_B, 7: CLAIM_I\n        # 8: CONCLUSION_B, 9: CONCLUSION_I\n        # 10: COUNTERCLAIM_B, 11: COUNTERCLAIM_I\n        # 12: REBUTTAL_B, 13: REBUTTAL_I\n        # 14: NOTHING i.e. O\n        ### NOTE THESE VALUES ARE DIVIDED BY 2 IN NEXT CODE LINE\n        pred = preds[id_num,]/2.0\n    \n        i = 0\n        while i<MAX_LEN:\n            prediction = []\n            start = pred[i]\n            if start in [0,1,2,3,4,5,6,7]:\n                prediction.append(word_map[i])\n                i += 1\n                if i>=MAX_LEN: break\n                while pred[i]==start+0.5:\n                    if not word_map[i] in prediction:\n                        prediction.append(word_map[i])\n                    i += 1\n                    if i>=MAX_LEN: break\n            else:\n                i += 1\n            prediction = [x for x in prediction if x!=-1]\n            if len(prediction)>4:\n                all_predictions.append( (n, target_map_rev[int(start)], \n                                ' '.join([str(x) for x in prediction]) ) )\n                \n    # MAKE DATAFRAME\n    df = pd.DataFrame(all_predictions)\n    df.columns = ['id','class','predictionstring']\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:44:30.105350Z","iopub.execute_input":"2023-05-06T06:44:30.105555Z","iopub.status.idle":"2023-05-06T06:44:30.120078Z","shell.execute_reply.started":"2023-05-06T06:44:30.105529Z","shell.execute_reply":"2023-05-06T06:44:30.118715Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"oof = get_preds( dataset='train', verbose=True, text_ids=IDS[valid_idx] )\noof.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:44:30.122586Z","iopub.execute_input":"2023-05-06T06:44:30.123128Z","iopub.status.idle":"2023-05-06T06:44:57.381492Z","shell.execute_reply.started":"2023-05-06T06:44:30.123085Z","shell.execute_reply":"2023-05-06T06:44:57.380805Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , ","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"             id     class                                   predictionstring\n0  50B3435E475B      Lead  3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20...\n1  50B3435E475B  Position             63 64 65 66 67 68 69 70 71 72 73 74 75\n2  50B3435E475B     Claim                76 77 78 79 80 81 82 83 84 85 86 87\n3  50B3435E475B  Evidence  88 89 90 91 92 93 94 95 96 97 98 99 100 101 10...\n4  50B3435E475B     Claim  162 163 164 165 166 167 168 169 170 171 172 17...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>class</th>\n      <th>predictionstring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50B3435E475B</td>\n      <td>Lead</td>\n      <td>3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50B3435E475B</td>\n      <td>Position</td>\n      <td>63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>50B3435E475B</td>\n      <td>Claim</td>\n      <td>76 77 78 79 80 81 82 83 84 85 86 87</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>50B3435E475B</td>\n      <td>Evidence</td>\n      <td>88 89 90 91 92 93 94 95 96 97 98 99 100 101 10...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>50B3435E475B</td>\n      <td>Claim</td>\n      <td>162 163 164 165 166 167 168 169 170 171 172 17...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('The following classes are present in oof preds:')\noof['class'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:44:57.382790Z","iopub.execute_input":"2023-05-06T06:44:57.383055Z","iopub.status.idle":"2023-05-06T06:44:57.391067Z","shell.execute_reply.started":"2023-05-06T06:44:57.383023Z","shell.execute_reply":"2023-05-06T06:44:57.389270Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"The following classes are present in oof preds:\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"array(['Lead', 'Position', 'Claim', 'Evidence', 'Concluding Statement',\n       'Counterclaim', 'Rebuttal'], dtype=object)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Compute Validation Metric\nThe following code is from Rob Mulla's excellent notebook [here][2]. Our LongFormer single fold model achieves CV score 0.633! Hooray!\n\n[2]: https://www.kaggle.com/robikscube/student-writing-competition-twitch","metadata":{}},{"cell_type":"code","source":"# CODE FROM : Rob Mulla @robikscube\n# https://www.kaggle.com/robikscube/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP / (TP + 0.5*(FP+FN))\n    return my_f1_score","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:44:57.392663Z","iopub.execute_input":"2023-05-06T06:44:57.393164Z","iopub.status.idle":"2023-05-06T06:44:57.408213Z","shell.execute_reply.started":"2023-05-06T06:44:57.393066Z","shell.execute_reply":"2023-05-06T06:44:57.407336Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# VALID DATAFRAME\nvalid = train.loc[train['id'].isin(IDS[valid_idx])]","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:44:57.409661Z","iopub.execute_input":"2023-05-06T06:44:57.410038Z","iopub.status.idle":"2023-05-06T06:44:57.434120Z","shell.execute_reply.started":"2023-05-06T06:44:57.410002Z","shell.execute_reply":"2023-05-06T06:44:57.433316Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"f1s = []\nCLASSES = oof['class'].unique()\nfor c in CLASSES:\n    pred_df = oof.loc[oof['class']==c].copy()\n    gt_df = valid.loc[valid['discourse_type']==c].copy()\n    f1 = score_feedback_comp(pred_df, gt_df)\n    print(c,f1)\n    f1s.append(f1)\nprint()\nprint('Overall',np.mean(f1s))","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:44:57.437391Z","iopub.execute_input":"2023-05-06T06:44:57.437602Z","iopub.status.idle":"2023-05-06T06:44:59.294662Z","shell.execute_reply.started":"2023-05-06T06:44:57.437577Z","shell.execute_reply":"2023-05-06T06:44:59.293814Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Lead 0.8063284233496999\nPosition 0.6841560234725578\nClaim 0.6057328285559762\nEvidence 0.6816788493279887\nConcluding Statement 0.7827050997782705\nCounterclaim 0.4854732895970009\nRebuttal 0.39030955585464333\n\nOverall 0.6337691528480197\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Infer Test Data\nWe will now infer the test data and create a submission. Our CV is 0.633, let's see what our LB is!","metadata":{}},{"cell_type":"code","source":"# GET TEST TEXT IDS\nfiles = os.listdir('../input/feedback-prize-2021/test')\nTEST_IDS = [f.replace('.txt','') for f in files if 'txt' in f]\nprint('There are',len(TEST_IDS),'test texts.')","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:44:59.298820Z","iopub.execute_input":"2023-05-06T06:44:59.299071Z","iopub.status.idle":"2023-05-06T06:44:59.312650Z","shell.execute_reply.started":"2023-05-06T06:44:59.299041Z","shell.execute_reply":"2023-05-06T06:44:59.311709Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"There are 5 test texts.\n","output_type":"stream"}]},{"cell_type":"code","source":"# CONVERT TEST TEXT TO TOKENS\ntest_tokens = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\ntest_attention = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\n\nfor id_num in range(len(TEST_IDS)):\n        \n    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n    n = TEST_IDS[id_num]\n    name = f'../input/feedback-prize-2021/test/{n}.txt'\n    txt = open(name, 'r').read()\n    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n                                   truncation=True, return_offsets_mapping=True)\n    test_tokens[id_num,] = tokens['input_ids']\n    test_attention[id_num,] = tokens['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:44:59.314349Z","iopub.execute_input":"2023-05-06T06:44:59.314817Z","iopub.status.idle":"2023-05-06T06:44:59.369252Z","shell.execute_reply.started":"2023-05-06T06:44:59.314763Z","shell.execute_reply":"2023-05-06T06:44:59.368246Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# INFER TEST TEXTS\np = model.predict([test_tokens, test_attention], \n                  batch_size=16, verbose=2)\nprint('Test predictions shape:',p.shape)\ntest_preds = np.argmax(p,axis=-1)","metadata":{"execution":{"iopub.status.busy":"2023-05-06T06:44:59.370893Z","iopub.execute_input":"2023-05-06T06:44:59.371431Z","iopub.status.idle":"2023-05-06T06:44:59.986467Z","shell.execute_reply.started":"2023-05-06T06:44:59.371393Z","shell.execute_reply":"2023-05-06T06:44:59.985657Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"1/1 - 1s\nTest predictions shape: (5, 1024, 15)\n","output_type":"stream"}]}]}